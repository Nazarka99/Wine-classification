{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91d43dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03878b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('wine.data', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a53f5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column names\n",
    "column_names = ['Type', 'Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium', 'Total phenols', 'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins', 'Color intensity', 'Hue', 'OD280/OD315 of diluted wines', 'Proline']\n",
    "\n",
    "# Set the column names of the DataFrame\n",
    "df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f819d6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>14.20</td>\n",
       "      <td>1.76</td>\n",
       "      <td>2.45</td>\n",
       "      <td>15.2</td>\n",
       "      <td>112</td>\n",
       "      <td>3.27</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.97</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.05</td>\n",
       "      <td>2.85</td>\n",
       "      <td>1450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Type  Alcohol  Malic acid   Ash  Alcalinity of ash  Magnesium  \\\n",
       "0       1    13.20        1.78  2.14               11.2        100   \n",
       "1       1    13.16        2.36  2.67               18.6        101   \n",
       "2       1    14.37        1.95  2.50               16.8        113   \n",
       "3       1    13.24        2.59  2.87               21.0        118   \n",
       "4       1    14.20        1.76  2.45               15.2        112   \n",
       "..    ...      ...         ...   ...                ...        ...   \n",
       "172     3    13.71        5.65  2.45               20.5         95   \n",
       "173     3    13.40        3.91  2.48               23.0        102   \n",
       "174     3    13.27        4.28  2.26               20.0        120   \n",
       "175     3    13.17        2.59  2.37               20.0        120   \n",
       "176     3    14.13        4.10  2.74               24.5         96   \n",
       "\n",
       "     Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0             2.65        2.76                  0.26             1.28   \n",
       "1             2.80        3.24                  0.30             2.81   \n",
       "2             3.85        3.49                  0.24             2.18   \n",
       "3             2.80        2.69                  0.39             1.82   \n",
       "4             3.27        3.39                  0.34             1.97   \n",
       "..             ...         ...                   ...              ...   \n",
       "172           1.68        0.61                  0.52             1.06   \n",
       "173           1.80        0.75                  0.43             1.41   \n",
       "174           1.59        0.69                  0.43             1.35   \n",
       "175           1.65        0.68                  0.53             1.46   \n",
       "176           2.05        0.76                  0.56             1.35   \n",
       "\n",
       "     Color intensity   Hue  OD280/OD315 of diluted wines  Proline  \n",
       "0               4.38  1.05                          3.40     1050  \n",
       "1               5.68  1.03                          3.17     1185  \n",
       "2               7.80  0.86                          3.45     1480  \n",
       "3               4.32  1.04                          2.93      735  \n",
       "4               6.75  1.05                          2.85     1450  \n",
       "..               ...   ...                           ...      ...  \n",
       "172             7.70  0.64                          1.74      740  \n",
       "173             7.30  0.70                          1.56      750  \n",
       "174            10.20  0.59                          1.56      835  \n",
       "175             9.30  0.60                          1.62      840  \n",
       "176             9.20  0.61                          1.60      560  \n",
       "\n",
       "[177 rows x 14 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed7356fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define the input features (X) and target variable (Y)\n",
    "X = np.array(df.drop('Type', axis=1))\n",
    "Y = np.array(df['Type'])\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "# Print the accuracy of the model\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "822a6e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "degrees = range(1, 9)\n",
    "accuracy_list = []\n",
    "mse_list = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Create a logistic regression model\n",
    "    model = LogisticRegression()\n",
    "\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    # Calculate the mean squared error of the model\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "# Plot the accuracy and mean squared error for each degree\n",
    "plt.plot(degrees, accuracy_list, label='Accuracy')\n",
    "plt.plot(degrees, mse_list, label='MSE')\n",
    "plt.xlabel('Degree')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83f377c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classification Accuracy: 0.86\n",
      "Random Forest Classification Accuracy: 0.97\n",
      "Neural Network Classification Accuracy: 0.92\n",
      "XGBoost Classification Accuracy: 0.94\n",
      "Logistic Regression Accuracy: 0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define the input features (X) and target variable (Y)\n",
    "X = np.array(df.drop('Type', axis=1))\n",
    "Y = np.array(df['Type'])\n",
    "\n",
    "# Shift the classes in Y from [1, 2, 3] to [0, 1, 2]\n",
    "Y = Y - 1\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create the models\n",
    "models = {\n",
    "    'Support Vector Classification': SVC(),\n",
    "    'Random Forest Classification': RandomForestClassifier(),\n",
    "    'Neural Network Classification': MLPClassifier(),\n",
    "    'XGBoost Classification': XGBClassifier(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "\n",
    "# Train the models and check their accuracy\n",
    "for name, model in models.items():\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "    # Print the accuracy of the model\n",
    "    print(f'{name} Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "360cf425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classification Accuracy: 0.78\n",
      "Random Forest Classification Accuracy: 0.97\n",
      "Neural Network Classification Accuracy: 0.42\n",
      "XGBoost Classification Accuracy: 0.94\n",
      "Logistic Regression Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Define the input features (X) and target variable (Y)\n",
    "X = np.array(df.drop('Type', axis=1))\n",
    "Y = np.array(df['Type'])\n",
    "\n",
    "# Shift the classes in Y from [1, 2, 3] to [0, 1, 2]\n",
    "Y = Y - 1\n",
    "\n",
    "\n",
    "# # Split the data into training and test sets\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create the models\n",
    "models = {\n",
    "    'Support Vector Classification': SVC(),\n",
    "    'Random Forest Classification': RandomForestClassifier(),\n",
    "    'Neural Network Classification': MLPClassifier(),\n",
    "    'XGBoost Classification': XGBClassifier(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "\n",
    "# Train the models and check their accuracy\n",
    "for name, model in models.items():\n",
    "    # Train the model on the training data\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "    # Print the accuracy of the model\n",
    "    print(f'{name} Accuracy: {accuracy:.2f}')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb7ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
